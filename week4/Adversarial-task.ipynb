{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating human faces with Adversarial Networks\n",
    "[![img](https://www.strangerdimensions.com/wp-content/uploads/2013/11/reception-robot.jpg)](http://payload.cargocollective.com/1/0/17293/393670/HumanTaxidermy01.jpg)\n",
    "\n",
    "This time we'll train a neural net to generate plausible human faces in all their subtlty: appearance, expression, accessories, etc. 'Cuz when us machines gonna take over Earth, there won't be any more faces left. We want to preserve this data for future iterations. Yikes...\n",
    "\n",
    "Based on Based on https://github.com/Lasagne/Recipes/pull/94 .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import grading\n",
    "import download_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "download_utils.link_week_4_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628dd722cf284831b490d7c739f4a750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "plt.rcParams.update({'axes.titlesize': 'small'})\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "#The following line fetches you two datasets: images, usable for autoencoder training and attributes.\n",
    "#Those attributes will be required for the final part of the assignment (applying smiles), so please keep them in mind\n",
    "from lfw_dataset import load_lfw_dataset \n",
    "data,attrs = load_lfw_dataset(dimx=36,dimy=36)\n",
    "\n",
    "#preprocess faces\n",
    "data = np.float32(data)/255.\n",
    "\n",
    "IMG_SHAPE = data.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13143, 36, 36, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22c8d69b438>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nESn/D6X3evyH0qk5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22c8d55a588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print random image\n",
    "plt.imshow(data[np.random.randint(data.shape[0])], cmap=\"gray\", interpolation=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative adversarial nets 101\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/torch/torch.github.io/master/blog/_posts/images/model.png\" width=320px height=240px>\n",
    "\n",
    "Deep learning is simple, isn't it? \n",
    "* build some network that generates the face (small image)\n",
    "* make up a __measure__ of __how good that face is__\n",
    "* optimize with gradient descent :)\n",
    "\n",
    "\n",
    "The only problem is: how can we engineers tell well-generated faces from bad? And i bet you we won't ask a designer for help. \n",
    "\n",
    "__If we can't tell good faces from bad, we delegate it to yet another neural network!__\n",
    "\n",
    "That makes the two of them:\n",
    "* __G__enerator - takes random noize for inspiration and tries to generate a face sample. \n",
    "  * Let's call him __G__(z), where z is a gaussian noize.\n",
    "* __D__iscriminator - takes a face sample and tries to tell if it's great or fake. \n",
    "  * Predicts the probability of input image being a __real face__\n",
    "  * Let's call him __D__(x), x being an image.\n",
    "  * __D(x)__ is a predition for real image and __D(G(z))__ is prediction for the face made by generator.\n",
    "\n",
    "Before we dive into training them, let's construct the two networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpu_options = tf.GPUOptions(allow_growth=True,per_process_gpu_memory_fraction=0.333)\n",
    "s = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras import layers as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "CODE_SIZE = 256\n",
    "\n",
    "generator = Sequential()\n",
    "generator.add(L.InputLayer([CODE_SIZE],name='noise'))\n",
    "generator.add(L.Dense(10*8*8, activation='elu'))\n",
    "\n",
    "generator.add(L.Reshape((8,8,10)))\n",
    "generator.add(L.Deconv2D(64,kernel_size=(5,5),activation='elu'))\n",
    "generator.add(L.Deconv2D(64,kernel_size=(5,5),activation='elu'))\n",
    "generator.add(L.UpSampling2D(size=(2,2)))\n",
    "generator.add(L.Deconv2D(32,kernel_size=3,activation='elu'))\n",
    "generator.add(L.Deconv2D(32,kernel_size=3,activation='elu'))\n",
    "generator.add(L.Deconv2D(32,kernel_size=3,activation='elu'))\n",
    "\n",
    "generator.add(L.Conv2D(3,kernel_size=3,activation=None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "noise (InputLayer)           (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 640)               164480    \n",
      "_________________________________________________________________\n",
      "reshape_6 (Reshape)          (None, 8, 8, 10)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_26 (Conv2DT (None, 12, 12, 64)        16064     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_27 (Conv2DT (None, 16, 16, 64)        102464    \n",
      "_________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_28 (Conv2DT (None, 34, 34, 32)        18464     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_29 (Conv2DT (None, 36, 36, 32)        9248      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_30 (Conv2DT (None, 38, 38, 32)        9248      \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 36, 36, 3)         867       \n",
      "=================================================================\n",
      "Total params: 320,835\n",
      "Trainable params: 320,835\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert generator.output_shape[1:] == IMG_SHAPE, \"generator must output an image of shape %s, but instead it produces %s\"%(IMG_SHAPE,generator.output_shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "* Discriminator is your usual convolutional network with interlooping convolution and pooling layers\n",
    "* The network does not include dropout/batchnorm to avoid learning complications.\n",
    "* We also regularize the pre-output layer to prevent discriminator from being too certain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "discriminator = Sequential()\n",
    "\n",
    "discriminator.add(L.InputLayer(IMG_SHAPE))\n",
    "\n",
    "discriminator.add(L.Conv2D(8, (3, 3)))\n",
    "discriminator.add(L.LeakyReLU(0.1))\n",
    "discriminator.add(L.Conv2D(16, (3, 3)))\n",
    "discriminator.add(L.LeakyReLU(0.1))\n",
    "discriminator.add(L.MaxPool2D())\n",
    "discriminator.add(L.Conv2D(32, (3, 3)))\n",
    "discriminator.add(L.LeakyReLU(0.1))\n",
    "discriminator.add(L.Conv2D(64, (3, 3)))\n",
    "discriminator.add(L.LeakyReLU(0.1))\n",
    "discriminator.add(L.MaxPool2D())\n",
    "\n",
    "discriminator.add(L.Flatten())\n",
    "discriminator.add(L.Dense(256,activation='tanh'))\n",
    "discriminator.add(L.Dense(2,activation=tf.nn.log_softmax))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 36, 36, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 34, 34, 8)         224       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 34, 34, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 32, 32, 16)        1168      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 32, 32, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 14, 14, 32)        4640      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 12, 12, 64)        18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 256)               590080    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 615,122\n",
      "Trainable params: 615,122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "We train the two networks concurrently:\n",
    "* Train __discriminator__ to better distinguish real data from __current__ generator\n",
    "* Train __generator__ to make discriminator think generator is real\n",
    "* Since discriminator is a differentiable neural network, we train both with gradient descent.\n",
    "\n",
    "![img](https://s24.postimg.org/cw4nognxx/gan.png)\n",
    "\n",
    "Training is done iteratively until discriminator is no longer able to find the difference (or until you run out of patience).\n",
    "\n",
    "\n",
    "### Tricks:\n",
    "* Regularize discriminator output weights to prevent explosion\n",
    "* Train generator with __adam__ to speed up training. Discriminator trains with SGD to avoid problems with momentum.\n",
    "* More: https://github.com/soumith/ganhacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noise = tf.placeholder('float32',[None,CODE_SIZE])\n",
    "real_data = tf.placeholder('float32',[None,]+list(IMG_SHAPE))\n",
    "\n",
    "logp_real = discriminator(real_data)\n",
    "\n",
    "generated_data =generator(noise)\n",
    "\n",
    "logp_gen = discriminator(generated_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'sequential_8/conv2d_14/BiasAdd:0' shape=(?, ?, ?, 3) dtype=float32>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'strided_slice_4:0' shape=(?,) dtype=float32>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logp_gen[:,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'pow_6:0' shape=(256, 2) dtype=float32>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator.layers[-1].kernel**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################\n",
    "#discriminator training#\n",
    "########################\n",
    "\n",
    "d_loss = -tf.reduce_mean(logp_real[:,1] + logp_gen[:,0])\n",
    "\n",
    "#regularize\n",
    "d_loss += tf.reduce_mean(discriminator.layers[-1].kernel**2)\n",
    "\n",
    "#optimize\n",
    "disc_optimizer =  tf.train.GradientDescentOptimizer(1e-3).minimize(d_loss,var_list=discriminator.trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################\n",
    "###generator training###\n",
    "########################\n",
    "\n",
    "g_loss =-tf.reduce_mean(logp_gen[:,1])\n",
    "\n",
    "gen_optimizer = tf.train.AdamOptimizer(1e-4).minimize(g_loss,var_list=generator.trainable_weights)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions\n",
    "Here we define a few helper functions that draw current data distributions and sample training batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_noise_batch(bsize):\n",
    "    return np.random.normal(size=(bsize, CODE_SIZE)).astype('float32')\n",
    "\n",
    "def sample_data_batch(bsize):\n",
    "    idxs = np.random.choice(np.arange(data.shape[0]), size=bsize)\n",
    "    return data[idxs]\n",
    "\n",
    "def sample_images(nrow,ncol, sharp=False):\n",
    "    images = generator.predict(sample_noise_batch(bsize=nrow*ncol))\n",
    "    if np.var(images)!=0:\n",
    "        images = images.clip(np.min(data),np.max(data))\n",
    "    for i in range(nrow*ncol):\n",
    "        plt.subplot(nrow,ncol,i+1)\n",
    "        if sharp:\n",
    "            plt.imshow(images[i].reshape(IMG_SHAPE),cmap=\"gray\", interpolation=\"none\")\n",
    "        else:\n",
    "            plt.imshow(images[i].reshape(IMG_SHAPE),cmap=\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "def sample_probas(bsize):\n",
    "    plt.title('Generated vs real data')\n",
    "    plt.hist(np.exp(discriminator.predict(sample_data_batch(bsize)))[:,1],\n",
    "             label='D(x)', alpha=0.5,range=[0,1])\n",
    "    plt.hist(np.exp(discriminator.predict(generator.predict(sample_noise_batch(bsize))))[:,1],\n",
    "             label='D(G(z))',alpha=0.5,range=[0,1])\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Main loop.\n",
    "We just train generator and discriminator in a loop and plot results once every N iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-1ee05085725c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mdisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0msample_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0msample_probas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-68-378fbe3d6c90>\u001b[0m in \u001b[0;36msample_images\u001b[1;34m(nrow, ncol, sharp)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_noise_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnrow\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mncol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrow\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mncol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mncol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mamin\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   2370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2371\u001b[0m     return _methods._amin(a, axis=axis,\n\u001b[1;32m-> 2372\u001b[1;33m                           out=out, **kwargs)\n\u001b[0m\u001b[0;32m   2373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_amin\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_amin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mumr_minimum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython import display\n",
    "from tqdm import tnrange\n",
    "\n",
    "for epoch in tnrange(50000):\n",
    "    \n",
    "    feed_dict = {\n",
    "        real_data:sample_data_batch(100),\n",
    "        noise:sample_noise_batch(100)\n",
    "    }\n",
    "    \n",
    "    for i in range(5):\n",
    "        s.run(disc_optimizer,feed_dict)\n",
    "    \n",
    "    s.run(gen_optimizer,feed_dict)\n",
    "    \n",
    "    if epoch %100==0:\n",
    "        display.clear_output(wait=True)\n",
    "        sample_images(2,3,True)\n",
    "        sample_probas(1000)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from submit_honor import submit_honor\n",
    "submit_honor((generator, discriminator), <YOUR_EMAIL>, <YOUR_TOKEN>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-4c48b5255474>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#Training for longer yields MUCH better results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0msample_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sample_images' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22c8d606cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#The network was trained for about 15k iterations. \n",
    "#Training for longer yields MUCH better results\n",
    "plt.figure(figsize=[16,24])\n",
    "sample_images(16,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
